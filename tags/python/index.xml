<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Miss Vidhyalakshmi Parthasarathy</title>
    <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/tags/python/</link>
    <description>Recent content in Python on Miss Vidhyalakshmi Parthasarathy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Jan 2022 10:58:08 -0400</lastBuildDate><atom:link href="https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project 1: Credit Risk Assessment FinTech Framework</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-1/</link>
      <pubDate>Sat, 01 Jan 2022 10:58:08 -0400</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-1/</guid>
      <description>This project represents the credit risk assessment dual framework of predicting the credit scores in the range of 300 (poor score and least trust) to 850 (exceptional score and most trust) and the forecasts of the credit default risk probability of the individuals; which can be utilized by the financial institutions like commercial banks and lending firms to evaluate the credit worthiness of their customers and the new applicants.</description>
    </item>
    
    <item>
      <title>Project 2: Bank Customers Churn Prediction</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-2/</link>
      <pubDate>Sat, 01 Jan 2022 10:58:08 -0400</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-2/</guid>
      <description>This project deals with the classification of the bank customers on whether a customer will leave the bank (i.e.; churn) or not.
  As part of this project; below steps of a Data Science Project Life-Cycle is being implemented.
 Data Exploration, Analysis and Visualisations Data Pre-processing Data Preparation for the Modelling Model Training Model Validation Optimized Model Selection based on Various Performance Metrics Deploying the Best Optimized Model into Unseen Test Data Evaluating the Optimized Model’s Performance Metrics    The business case and the problem statement to determine the churn status of the bank customers are explored, trained and validated on 7 different classification algorithms/models as listed below and the best finalized optimized model is selected based on the various performance metrics namely accuracy, precision, recall and f1-score.</description>
    </item>
    
    <item>
      <title>Project 3: Content Management Web System With PostgreSQL Database, APIs and Cloud Deployment</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-3/</link>
      <pubDate>Sat, 01 Jan 2022 10:58:08 -0400</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-3/</guid>
      <description>This project repository includes a Web-based Content (Articles) Management System/Application related to Data Science Learning and Career Journey with User Registration, Login functionalities using Python, Flask Web Framework, HTML, PostgreSQL database and Heroku Cloud Server.
  This application is implemented and deployed in Heroku Cloud Server.
  The application functionalities include below:
 New User Registration (First Name, Last Name, Email Address, Gender, Language, Password) Existing User Login User Management (Modify/Edit Existing User Details and Delete Existing User) New Article/Content Creation View and Retrieve list of Users and the Articles References - covering various public website links External References - covering mathematical functions for quick references    This application includes REST APIs implementation and cloud based functionalities covering below:</description>
    </item>
    
    <item>
      <title>Project 4: Exploratory Data Analysis (EDA) and Data Cleaning of Spark Fund Investments</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-4/</link>
      <pubDate>Sat, 01 Jan 2022 10:58:08 -0400</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-4/</guid>
      <description>This project deals with the Exploratory Data Analysis and the Data Cleaning of the various Companies Data for Spark Fund Investment, along with the Profiling Report.  A detailed HTML version of the profiling reports for the Companies Data and the Rounds Data can be viewed from the reports folder of the project&amp;rsquo;s github repository. NOTE: HTML Reports can be accessed in a user-friendly readable format via a web browser; after copying it into a local folder of the desktop/laptop.</description>
    </item>
    
    <item>
      <title>Project 8: Audio Digits Classification using Convolutional Neural Network</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-8/</link>
      <pubDate>Sat, 01 Jan 2022 10:58:08 -0400</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-8/</guid>
      <description>The project cover the basic solution and the Advanced Solution as given below based on Audio Feature Extraction Method named &amp;ldquo;Mel-frequency cepstral coefficients (MFCC)&amp;rdquo; and Deep Learning Convolutional Neural Network (CNN).
  Basic Solution: Includes designing, building, training, validation and testing a model created to recognise numerals from 0 to 9 in the audio files.
  Advanced Solution: Includes implementing the solution to predict the numeral based on a new audio test file.</description>
    </item>
    
    <item>
      <title>Project 9: Network Analysis and Identification of Fraudulent and Trusted Users in the Bitcoin Network</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-9/</link>
      <pubDate>Sat, 01 Jan 2022 10:58:08 -0400</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-9/</guid>
      <description>This project deals with the network analysis covering 4 different problem statements and use cases using python NetworkX package, Gephi network analysis tool and Microsoft excel.
  Dataset: Dataset includes Bitcoin Trade Transactions for the period between 2011 to 2016.
  Dataset Representation: Bitcoin Trade Transactions -&amp;gt; Attributes (Rater, Ratee, Rating and Timestamp)
  Network Formation: For every trade transaction between 2 users in the Bitcoin Network; ratings are recorded and tracked in the system with the corresponding timestamp (Directed Network).</description>
    </item>
    
    <item>
      <title>Project 10: Network Analytics of the Financial Asset Class Prices</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-10/</guid>
      <description>This project covers the network analysis and the implementation of the financial asset classes prices using the Python’s NetworkX library with multiple interactive visualisations represented in a user-friendly manner.
  The implementation is dealt and approached as a real-world business use case and problem statement.
  Tools &amp;amp; Technologies: Python (networkX, plotly, pandas, numpy, matplotlib), GitHub
  Financial Assets Prices : Correlation  Link to Project&amp;rsquo;s GitHub Repository</description>
    </item>
    
    <item>
      <title>Project 11: NLP - Classification for Deception Detection of the Amazon Reviews</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-11/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-11/</guid>
      <description>This project covers the implementation of the classification for Deception Detection of the Amazon text reviews private dataset using TF-IDF (Term Frequency–Inverse Document Frequency).
  Tools &amp;amp; Technologies: Python (nltk, pandas, numpy, matplotlib), Github
  Amazon Text Reviews : Real versus Fake  Link to Project&amp;rsquo;s GitHub Repository</description>
    </item>
    
    <item>
      <title>Project 12: NLP - CRF Sequence BIO Tagger Implementation of the Movie Trivia</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-12/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-12/</guid>
      <description>This project covers the implementation of the Conditional Random Field (CRF) sequence tagger for the movie trivia questions and answers private dataset with BIO (Beginning, Inside, Outside) tagging format.
  Tools &amp;amp; Technologies: Python (nltk, pycrfsuite, pandas, numpy, matplotlib), Github
   Link to Project&amp;rsquo;s GitHub Repository</description>
    </item>
    
    <item>
      <title>Project 13: NLP - Distributional Semantics Implementation of the Corpus Data with Sampling Methods</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-13/</guid>
      <description>This project covers the distributional semantics to investigate how some words in the English language changed over the course of the last two centuries 2000 and 2010 using a private Corpus of Historical American English (COHA) dataset.
  The project explores and implements various sampling methods as listed below:
 N-Gram CBOW SkipGram Word2Vec    Tools &amp;amp; Technologies: Python (nltk, word2vec, pandas, numpy, matplotlib), GitHub</description>
    </item>
    
    <item>
      <title>Project 14: NLP - Topic Modelling of the BBC Articles using Unsupervised LDA Model</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-14/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-14/</guid>
      <description>This project covers the implementation of the Topic Modelling of the BBC Articles Dataset using the Unsupervised Latent Dirichlet Allocation (LDA) Model.
  Tools &amp;amp; Technologies: Python (nltk, langdetect, genism, pandas, numpy, matplotlib), GitHub
  Distribution of the Topics in the Various Articles  Link to Project&amp;rsquo;s GitHub Repository</description>
    </item>
    
    <item>
      <title>Project 16: Apache Hadoop HDFS MapReduce Jobs with Replication and Repartition Joins for NASDAQ Data</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-16/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-16/</guid>
      <description>This project includes the development of a MapReduce python script with and without combiner from scratch for the implementation of the various replication and repartition join strategies; for a big data job for the “NASDAQ” private dataset; listed with the NASDAQ daily stock variations between 1970 and 2010.
  The data files are stored in the Hadoop HDFS and 2 jobs are executed in the Hadoop Cluster.</description>
    </item>
    
    <item>
      <title>Project 17: Apache Spark RDD Computations with Transformations and Actions for Gutenberg Data</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-17/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-17/</guid>
      <description>This project covers the development of Spark RDD computations from scratch using python’s pyspark package and regular expressions functions for the “Gutenberg” private data files; containing hundreds of books downloaded from the project Gutenberg, written in different languages.
  Implemented the use of basic transformations (namely flatMap, map, reduceByKey) and actions on the RDDs and submitted spark jobs to the cluster.
  Implemented solutions to the below questions and scenarios:</description>
    </item>
    
    <item>
      <title>Project 18: Data Warehousing and OLAP Implementation of IBRD Balance Sheet Data</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-18/</guid>
      <description>This project covers the Implementation of the Data Warehousing and On-line Analytical Processing (OLAP) concepts of data cubes, data cube measures, OLAP operations and data cube computations using the International Bank for Reconstruction and Development (IBRD) Balance Sheet private dataset.
  The implementation is executed using Python and its various packages namely cubes and sqlalchemy.
  The solution includes the following:
 Indexing the OLAP data using bitmap indices Creation of base tables Creation of bitmap index tables Creation of the data cube model in the JSON file format by defining the aggregate functions Creation of the data cube Computation of the results for the various aggregate measures as defined in the data cube    Tools &amp;amp; Technologies: Python (data cubes, sqlalchemy, pandas, numpy, matplotlib), GitHub</description>
    </item>
    
    <item>
      <title>Project 19: Web Scraping of Web Pages and Web Tables using Python Beautiful Soup</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-19/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-19/</guid>
      <description>This project covers the Implementation of the Web Mining and Web Scraping of the 2 public website pages with html tables and 2 QMUL’s internal website pages with html table using Python’s “Beautiful Soup” module for web scraping, pandas, numpy, seaborn and matplotlib libraries.
  The solution includes the following:
 Scraping the specific contents from the web page html tables as per the custom filter requisites Converted the scraped table contents into a readable pandas dataframe table with customised columns Performed data cleaning steps to remove any unnecessary characters from the pandas table Exported the clean table data into a csv file for all the 4 web pages individually    Tools &amp;amp; Technologies: Python (beautiful soup, pandas, numpy, matplotlib), GitHub</description>
    </item>
    
    <item>
      <title>Project 20: House Prices Anomaly Detection using Proximity-Based KNN Method</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-20/</guid>
      <description>This project covers the Implementation of the Outlier Detection using the proximity-based method of k-nearest neighbors to calculate the outlier scores on the”house prices” dataset; with the inclusion of the data pre-processing steps of z-score normalisation and PCA dimensionality reduction techniques.
  The implementation is executed using Python’s libraries namely pandas, numpy, matplotlib, sklearn and scipy.
  The solution includes the computation of the Euclidean Distance to further detect the top 3 outlier houses with the highest prices when compared with the average house price.</description>
    </item>
    
    <item>
      <title>Project 21: Python Data Analysis</title>
      <link>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-21/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tech-with-vidhya.github.io/Vidhyalakshmi_Parthasarathy_Portfolio/post/project-21/</guid>
      <description>This project deals with the Exploratory Data Analysis of the Pokemon Data using Python Pandas Library.
  Tools &amp;amp; Technologies: Python, GitHub
  Pokemon Data Analysis : Type 1 Stats Based on the Defense  Pokemon Data Analysis : Type 2 Stats Based on the Speed  Pokemon Data Analysis : Legendary Stats  Link to Project&amp;rsquo;s GitHub Repository</description>
    </item>
    
  </channel>
</rss>
